{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3cdf0d",
   "metadata": {},
   "source": [
    "# Named-entity Recognition (NER) \n",
    "\n",
    "Named entity recognition is a fundamental task in information extraction from textual documents. While named entities originally corresponded to real-world entities with names (named entities), this concept has been extended to any type of information: it is possible to extract chemical molecules, product numbers, amounts, addresses, etc. In this practical assignment, we will use several named entity extraction libraries in French on a small corpus. The objective is not to train the best possible model, but to test the use of each of these libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7cf35",
   "metadata": {},
   "source": [
    "## The AdminSet dataset\n",
    "The AdminSet dataset is a corpus of administrative documents in French produced by automatic character recognition and manually annotated with named entities. This corpus is quite difficult because the document recognition process produces noisy text (errors due to layout, recognition, fonts, etc.).\n",
    "\n",
    "The paper describing the dataset is available [here](https://hal.science/hal-04855066v1/file/AdminSet_et_AdminBERT__version___preprint.pdf).\n",
    "\n",
    "The corpus is available on HuggingFace: [Adminset-NER](https://huggingface.co/datasets/taln-ls2n/Adminset-NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('taln-ls2n/Adminset-NER')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62af8c6",
   "metadata": {},
   "source": [
    "#### Question\n",
    "> * Compute descriptive statistics on the texts  for each split (train, dev)\n",
    "> * Compute descriptive statistics on the entities for each split (train, dev)\n",
    "> * Compare with the statistics reported in the paper (Table 2)\n",
    "> * Display a couple of random texts with their entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics on the number of token in train and validation : min, max, mean std, median\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebbac3",
   "metadata": {},
   "source": [
    "### Creation of the splits\n",
    "\n",
    "The train_test_split() function from huggingface allow to split a dataset randomly in 2 parts : https://huggingface.co/docs/datasets/v4.5.0/process#split\n",
    "\n",
    "The ```spacy_utils.py``` file contains functions to save a dataset in text format (```save_text```, usefull for inspection), BIO format (```save_bio```) and spacy format (```save_docbin```).\n",
    "\n",
    "#### Questions\n",
    ">* Using the split function, create a train/dev/test split corresponding to the proportions reported in the paper\n",
    ">* Save the sets in a corpus directory, in text, bio and docbin formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_utils import save_bio, save_text, save_docbin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b05a4b",
   "metadata": {},
   "source": [
    "### Testing spaCy pre-trained NER models\n",
    "\n",
    "spaCy comes with a several pretrained models for many languages. For French, 4 models are provided : https://spacy.io/models/fr\n",
    "\n",
    "To apply a pretrained model to dataset, use : \n",
    "- ```nlp = spacy.load(MODEL_NAME)``` to load the model. You need to download it first with \"spacy download MODEL_NAME\"\n",
    "- ```DocBin().from_disk()``` to load a dataset in spaCy format from the disk\n",
    "- ```doc_bin.get_docs(nlp.vocab)``` to convert the dataset from binary to text format\n",
    "- ```nlp(doc.text)```to apply the NER model to a text\n",
    "\n",
    "To evaluate the prediction, you can use the spaCy [Scorer](https://spacy.io/api/scorer)\n",
    "- ```scorer.score(examples)``` where examples is a list of spaCy ```Example(prediction, reference)````\n",
    "\n",
    "#### Question\n",
    "\n",
    ">* Using a spaCy pretrained model for French, evaluate its performace for NER prediction on the train, dev and test sets\n",
    ">* Compare this model to results reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable # optional but nice\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a358f45",
   "metadata": {},
   "source": [
    "### Training a custom spaCy model\n",
    "\n",
    "The training of a cupstom spaCy NER model can be done both with the command line interface (cli) or in a python script. Using the cli is ususally more optimzed. All the configuration of the training is defined in a coniguration file, which is a good practice for documentation, tracing and reproducibility.\n",
    "\n",
    "The configuration file can be generated on line using the [Quickstart](https://spacy.io/usage/training#quickstart)\n",
    "\n",
    "<img src=\"images/spacy_quickstart.jpg\" width=\"600\" >\n",
    "\n",
    "You can run the training process as a script using the train function (https://spacy.io/usage/training#api-train), specifying the configuration file and the directory in which to save the model as parameters. Once the training is complete, the best and last models are saved in the directory.\n",
    "\n",
    "#### Question\n",
    "> * Generate a training configuration file for a NER in French\n",
    "> * Add the correct path to the training and dev sets generated previously\n",
    "> * train a NER model\n",
    "> * Evaluate the model on the train, dev et test sets. Compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from spacy.cli.train import train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34932c83",
   "metadata": {},
   "source": [
    "### Zero-shot NER prediction with GLiNER\n",
    "\n",
    "\n",
    "[GLiNER](https://github.com/fastino-ai/GLiNER2/tree/main)  is a library that provides models for zero-shot named entity recognition. This means that[structured information extraction](https://github.com/fastino-ai/GLiNER2/blob/main/tutorial/3-json_extraction.md)structured information extraction, which means that the extracted information can be organised in a structured JSON format. GLiNER does not provide the location of entities in the text by default, but you can configure the model to output this information (```include_spans=True```). Finally, GLiNER enables entities to be overlapped and nested, which is not supported by the spaCy scorer. The spaCy [filter_spans](https://spacy.io/api/top-level#util.filter_spans) function can be used to remove overlapping entities for evaluation.\n",
    "\n",
    "#### Question\n",
    "> * Define the entities to extract from the text.\n",
    "> * Apply GLiNER on the dev and test sets\n",
    "> * Evaluate the models on the dev and test sets and compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner2 import GLiNER2\n",
    "extractor = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\")\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.blank(\"fr\")  # tokenizer only\n",
    "\n",
    "doc_bin = DocBin().from_disk(\"REPLACE\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "label_map = {\n",
    "    # Define the entities here\n",
    "}\n",
    "\n",
    "gliner_labels = list(label_map.values())\n",
    "reverse_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "examples = []\n",
    "\n",
    "for gold_doc in gold_docs:\n",
    "    text = gold_doc.text\n",
    "    \n",
    "    predictions = extractor.extract_entities(text, gliner_labels, include_spans=True)\n",
    "    pred_doc = nlp.make_doc(text)\n",
    "\n",
    "    spans = []\n",
    "    for gliner_label, entities in predictions[\"entities\"].items():\n",
    "        spacy_label = reverse_map.get(gliner_label)\n",
    "        if not spacy_label:\n",
    "            continue\n",
    "        for ent in entities:\n",
    "            start = ent[\"start\"]\n",
    "            end = ent[\"end\"]\n",
    "\n",
    "            span = pred_doc.char_span(start, end, label=spacy_label)\n",
    "            if span:\n",
    "                spans.append(span)\n",
    "\n",
    "    spans = filter_spans(spans)\n",
    "    pred_doc.ents = spans\n",
    "    examples.append(Example(pred_doc, gold_doc))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
