{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3cdf0d",
   "metadata": {},
   "source": [
    "# Named-entity Recognition (NER) \n",
    "\n",
    "Named entity recognition is a fundamental task in information extraction from textual documents. While named entities originally corresponded to real-world entities with names (named entities), this concept has been extended to any type of information: it is possible to extract chemical molecules, product numbers, amounts, addresses, etc. In this practical assignment, we will use several named entity extraction libraries in French on a small corpus. The objective is not to train the best possible model, but to test the use of each of these libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7cf35",
   "metadata": {},
   "source": [
    "## The AdminSet dataset\n",
    "The AdminSet dataset is a corpus of administrative documents in French produced by automatic character recognition and manually annotated with named entities. This corpus is quite difficult because the document recognition process produces noisy text (errors due to layout, recognition, fonts, etc.).\n",
    "\n",
    "The paper describing the dataset is available [here](https://hal.science/hal-04855066v1/file/AdminSet_et_AdminBERT__version___preprint.pdf).\n",
    "\n",
    "The corpus is available on HuggingFace: [Adminset-NER](https://huggingface.co/datasets/taln-ls2n/Adminset-NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cdb46b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nysarakpy/github/NLP-Course-Ensae/Lab/nlp-lab3-ner/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 729\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 85\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('taln-ls2n/Adminset-NER')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62af8c6",
   "metadata": {},
   "source": [
    "#### Question\n",
    "> * Compute descriptive statistics on the texts  for each split (train, dev)\n",
    "> * Compute descriptive statistics on the entities for each split (train, dev)\n",
    "> * Compare with the statistics reported in the paper (Table 2)\n",
    "> * Display a couple of random texts with their entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db4b2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fin, Proc√®s-Verbal, Conseil, communautaire, d...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Monsieur, MORLET, excuse, Monsieur, Christoph...</td>\n",
       "      <td>[B-PER, I-PER, O, B-PER, I-PER, I-PER, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Monsieur, MORLET, annonce, le, d√©c√®s, de, Mon...</td>\n",
       "      <td>[B-PER, I-PER, O, O, O, O, B-PER, I-PER, I-PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Commentaires, ,, d√©bat, Constatant, qu'il, n'...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Page, 4, sur, 15, &lt;, page, &gt;, 4, &lt;, /, page, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [fin, Proc√®s-Verbal, Conseil, communautaire, d...   \n",
       "1  [Monsieur, MORLET, excuse, Monsieur, Christoph...   \n",
       "2  [Monsieur, MORLET, annonce, le, d√©c√®s, de, Mon...   \n",
       "3  [Commentaires, ,, d√©bat, Constatant, qu'il, n'...   \n",
       "4  [Page, 4, sur, 15, <, page, >, 4, <, /, page, ...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [B-PER, I-PER, O, B-PER, I-PER, I-PER, O, O, O...  \n",
       "2  [B-PER, I-PER, O, O, O, O, B-PER, I-PER, I-PER...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(ds['train'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfe49bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[et, L‚ÄôOffice, Communautaire, d‚ÄôAnimations, et...</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Sign√©, le, 22, f√©vrier, 2024, Re√ßu, au, Contr...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Re√ßu, au, Contr√¥le, de, l√©galit√©, le, 12, d√©c...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Etaient, absents, et, repr√©sent√©s, Mesdames, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-PER, I-PER, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Commune, d'Ollioules, -, Departement, du, Var...</td>\n",
       "      <td>[B-LOC, I-LOC, O, B-LOC, I-LOC, I-LOC, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [et, L‚ÄôOffice, Communautaire, d‚ÄôAnimations, et...   \n",
       "1  [Sign√©, le, 22, f√©vrier, 2024, Re√ßu, au, Contr...   \n",
       "2  [Re√ßu, au, Contr√¥le, de, l√©galit√©, le, 12, d√©c...   \n",
       "3  [Etaient, absents, et, repr√©sent√©s, Mesdames, ...   \n",
       "4  [Commune, d'Ollioules, -, Departement, du, Var...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, B-PER, I-PER, O, O, B...  \n",
       "4  [B-LOC, I-LOC, O, B-LOC, I-LOC, I-LOC, O, O, O...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.DataFrame(ds['validation'])\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f0642f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((729, 2), (85, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31e1f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fin, Proc√®s-Verbal, Conseil, communautaire, d...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Monsieur, MORLET, excuse, Monsieur, Christoph...</td>\n",
       "      <td>[B-PER, I-PER, O, B-PER, I-PER, I-PER, O, O, O...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Monsieur, MORLET, annonce, le, d√©c√®s, de, Mon...</td>\n",
       "      <td>[B-PER, I-PER, O, O, O, O, B-PER, I-PER, I-PER...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Commentaires, ,, d√©bat, Constatant, qu'il, n'...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Page, 4, sur, 15, &lt;, page, &gt;, 4, &lt;, /, page, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [fin, Proc√®s-Verbal, Conseil, communautaire, d...   \n",
       "1  [Monsieur, MORLET, excuse, Monsieur, Christoph...   \n",
       "2  [Monsieur, MORLET, annonce, le, d√©c√®s, de, Mon...   \n",
       "3  [Commentaires, ,, d√©bat, Constatant, qu'il, n'...   \n",
       "4  [Page, 4, sur, 15, <, page, >, 4, <, /, page, ...   \n",
       "\n",
       "                                            ner_tags  n_tokens  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...        63  \n",
       "1  [B-PER, I-PER, O, B-PER, I-PER, I-PER, O, O, O...        24  \n",
       "2  [B-PER, I-PER, O, O, O, O, B-PER, I-PER, I-PER...        31  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER...        18  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...        41  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute statistics on the number of token in train and validation : min, max, mean std, median\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# for train set\n",
    "train_df[\"n_tokens\"] = train_df[\"tokens\"].apply(len)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8130f2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    729.000000\n",
       "mean      63.367627\n",
       "std       52.705843\n",
       "min       15.000000\n",
       "25%       30.000000\n",
       "50%       45.000000\n",
       "75%       75.000000\n",
       "max      379.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"n_tokens\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53546bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"n_tokens\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ccf51c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 50.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     85.000000\n",
       "mean      79.835294\n",
       "std       68.679006\n",
       "min       19.000000\n",
       "25%       35.000000\n",
       "50%       50.000000\n",
       "75%       86.000000\n",
       "max      352.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for validation set\n",
    "val_df[\"n_tokens\"] = val_df[\"tokens\"].apply(len)\n",
    "\n",
    "print(\"Median:\", val_df[\"n_tokens\"].median())\n",
    "val_df[\"n_tokens\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083e191",
   "metadata": {},
   "source": [
    "**Table2 of the paper**\n",
    "\n",
    "<img src=\"images/paper_table2.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eb2fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entity tokens: 4983\n",
      "Number of entity labels: 6\n",
      "ner_tags\n",
      "I-ORG    1476\n",
      "I-PER    1092\n",
      "B-ORG     770\n",
      "B-PER     764\n",
      "B-LOC     454\n",
      "I-LOC     427\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_tags = train_df[\"ner_tags\"].explode()\n",
    "\n",
    "# remove \"O\"\n",
    "train_entities = train_tags[train_tags != \"O\"]\n",
    "\n",
    "print(\"Total entity tokens:\", len(train_entities))\n",
    "print(\"Number of entity labels:\", train_entities.nunique())\n",
    "print(train_entities.value_counts().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f7a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entity tokens: 694\n",
      "Number of entity labels: 6\n",
      "ner_tags\n",
      "I-ORG    203\n",
      "I-PER    138\n",
      "B-PER    124\n",
      "B-ORG    123\n",
      "I-LOC     54\n",
      "B-LOC     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "val_tags = val_df[\"ner_tags\"].explode()\n",
    "\n",
    "val_entities = val_tags[val_tags != \"O\"]\n",
    "\n",
    "print(\"Total entity tokens:\", len(val_entities))\n",
    "print(\"Number of entity labels:\", val_entities.nunique())\n",
    "print(val_entities.value_counts().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe10ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities: 1988\n",
      "Entity types: ner_tags\n",
      "ORG    770\n",
      "PER    764\n",
      "LOC    454\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_b_entities = train_entities[train_entities.str.startswith(\"B-\")]\n",
    "\n",
    "print(\"Total entities:\", len(train_b_entities))\n",
    "print(\"Entity types:\", train_b_entities.str[2:].value_counts().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4b35b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities: 299\n",
      "Entity types: ner_tags\n",
      "PER    124\n",
      "ORG    123\n",
      "LOC     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "val_b_entities = val_entities[val_entities.str.startswith(\"B-\")]\n",
    "\n",
    "print(\"Total entities:\", len(val_b_entities))\n",
    "print(\"Entity types:\", val_b_entities.str[2:].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b001706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Article 3 : En contrepartie , la Direction de l‚ÄôAction Socioculturelle s'engage √† faire b√©n√©ficier aux adh√©rents du Comit√© Social et √âconomique Airbus Defence and Space Toulouse uniquement de tarifs sp√©cifiques ¬´ Comit√© d‚ÄôEntreprise ¬ª pour l‚Äôachat de place de spectacles des centres culturels de la Direction de l‚ÄôAction Socioculturelle de Toulouse .\n",
      "\n",
      "Entities:\n",
      "l‚ÄôAction Socioculturelle -> ORG\n",
      "Comit√© Social et √âconomique Airbus Defence and Space Toulouse -> ORG\n",
      "l‚ÄôAction Socioculturelle de Toulouse -> ORG\n",
      "Text:\n",
      "Si le dossier est retenu , une convention sera √©tablie et communiqu√©e au porteur de projet pr√©cisant : 1 La nature , la dur√©e et l‚Äôobjet de l‚Äôintervention de la CAPB .\n",
      "\n",
      "Entities:\n",
      "CAPB . -> ORG\n"
     ]
    }
   ],
   "source": [
    "sample = train_df.sample(2)\n",
    "\n",
    "for i, row in sample.iterrows():\n",
    "    print(\"Text:\")\n",
    "    print(\" \".join(row[\"tokens\"]))\n",
    "    \n",
    "    print(\"\\nEntities:\")\n",
    "    \n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "    \n",
    "    for token, tag in zip(row[\"tokens\"], row[\"ner_tags\"]):\n",
    "        \n",
    "        if tag.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                print(\" \".join(current_entity), \"->\", current_label)\n",
    "            current_entity = [token]\n",
    "            current_label = tag[2:]\n",
    "        \n",
    "        elif tag.startswith(\"I-\"):\n",
    "            current_entity.append(token)\n",
    "        \n",
    "        else:\n",
    "            if current_entity:\n",
    "                print(\" \".join(current_entity), \"->\", current_label)\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "    \n",
    "    if current_entity:\n",
    "        print(\" \".join(current_entity), \"->\", current_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebbac3",
   "metadata": {},
   "source": [
    "### Creation of the splits\n",
    "\n",
    "The train_test_split() function from huggingface allow to split a dataset randomly in 2 parts : https://huggingface.co/docs/datasets/v4.5.0/process#split\n",
    "\n",
    "The ```spacy_utils.py``` file contains functions to save a dataset in text format (```save_text```, usefull for inspection), BIO format (```save_bio```) and spacy format (```save_docbin```).\n",
    "\n",
    "#### Questions\n",
    ">* Using the split function, create a train/dev/test split corresponding to the proportions reported in the paper\n",
    ">* Save the sets in a corpus directory, in text, bio and docbin formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07bc8ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy_utils import save_bio, save_text, save_docbin\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "full_ds = concatenate_datasets([ds[\"train\"], ds[\"validation\"]])\n",
    "full_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47c37959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((583, 2), (146, 2), (85, 2))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from paper\n",
    "train_ds = full_ds.select(range(0, 583))\n",
    "dev_ds   = full_ds.select(range(583, 583 + 146))\n",
    "test_ds  = full_ds.select(range(583 + 146, 814))\n",
    "\n",
    "train_ds.shape, dev_ds.shape, test_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7585e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving text to corpus/train.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:00<00:00, 9137.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.txt\n",
      "Saving text to corpus/dev.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 13776.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.txt\n",
      "Saving text to corpus/test.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 9229.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.txt\n",
      "Saving BIO text to corpus/train.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:00<00:00, 9312.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.bio\n",
      "Saving BIO text to corpus/dev.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 18575.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.bio\n",
      "Saving BIO text to corpus/test.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 12294.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.bio\n",
      "Creating corpus/train.spacy with 583 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:00<00:00, 3595.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.spacy\n",
      "Creating corpus/dev.spacy with 146 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 4539.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.spacy\n",
      "Creating corpus/test.spacy with 85 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 2793.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy_utils import save_bio, save_text, save_docbin\n",
    "\n",
    "# save the datasets in different formats\n",
    "save_text(train_ds, \"corpus/train.txt\")\n",
    "save_text(dev_ds, \"corpus/dev.txt\")\n",
    "save_text(test_ds, \"corpus/test.txt\")\n",
    "\n",
    "save_bio(train_ds, \"corpus/train.bio\")\n",
    "save_bio(dev_ds, \"corpus/dev.bio\")\n",
    "save_bio(test_ds, \"corpus/test.bio\")\n",
    "\n",
    "save_docbin(train_ds, \"corpus/train.spacy\")\n",
    "save_docbin(dev_ds, \"corpus/dev.spacy\")\n",
    "save_docbin(test_ds, \"corpus/test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b05a4b",
   "metadata": {},
   "source": [
    "### Testing spaCy pre-trained NER models\n",
    "\n",
    "spaCy comes with a several pretrained models for many languages. For French, 4 models are provided : https://spacy.io/models/fr\n",
    "\n",
    "To apply a pretrained model to dataset, use : \n",
    "- ```nlp = spacy.load(MODEL_NAME)``` to load the model. You need to download it first with \"spacy download MODEL_NAME\"\n",
    "- ```DocBin().from_disk()``` to load a dataset in spaCy format from the disk\n",
    "- ```doc_bin.get_docs(nlp.vocab)``` to convert the dataset from binary to text format\n",
    "- ```nlp(doc.text)```to apply the NER model to a text\n",
    "\n",
    "To evaluate the prediction, you can use the spaCy [Scorer](https://spacy.io/api/scorer)\n",
    "- ```scorer.score(examples)``` where examples is a list of spaCy ```Example(prediction, reference)````\n",
    "\n",
    "#### Question\n",
    "\n",
    ">* Using a spaCy pretrained model for French, evaluate its performace for NER prediction on the train, dev and test sets\n",
    ">* Compare this model to results reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "642b8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6a2bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:06<00:00, 93.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.19733252500757806\n",
      "Recall: 0.4033457249070632\n",
      "F1: 0.26501119478933444\n"
     ]
    }
   ],
   "source": [
    "# for train dataset\n",
    "doc_bin = DocBin().from_disk(\"corpus/train.spacy\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "examples = []\n",
    "\n",
    "for gold_doc in tqdm(gold_docs):\n",
    "    pred_doc = nlp(gold_doc.text)\n",
    "    examples.append(Example(pred_doc, gold_doc))\n",
    "\n",
    "scorer = Scorer()\n",
    "train_scores = scorer.score(examples)\n",
    "\n",
    "print(\"Precision:\", train_scores[\"ents_p\"])\n",
    "print(\"Recall:\", train_scores[\"ents_r\"])\n",
    "print(\"F1:\", train_scores[\"ents_f\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0c8d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:01<00:00, 95.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1683599419448476\n",
      "Recall: 0.31016042780748665\n",
      "F1: 0.21825023518344308\n"
     ]
    }
   ],
   "source": [
    "# for dev dataset\n",
    "doc_bin = DocBin().from_disk(\"corpus/dev.spacy\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "examples = []\n",
    "\n",
    "for gold_doc in tqdm(gold_docs):\n",
    "    pred_doc = nlp(gold_doc.text)\n",
    "    examples.append(Example(pred_doc, gold_doc))\n",
    "\n",
    "scorer = Scorer()\n",
    "dev_scores = scorer.score(examples)\n",
    "\n",
    "print(\"Precision:\", dev_scores[\"ents_p\"])\n",
    "print(\"Recall:\", dev_scores[\"ents_r\"])\n",
    "print(\"F1:\", dev_scores[\"ents_f\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63abb709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:01<00:00, 82.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1555944055944056\n",
      "Recall: 0.2976588628762542\n",
      "F1: 0.20436280137772675\n"
     ]
    }
   ],
   "source": [
    "# for testset\n",
    "doc_bin = DocBin().from_disk(\"corpus/test.spacy\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "examples = []\n",
    "\n",
    "for gold_doc in tqdm(gold_docs):\n",
    "    pred_doc = nlp(gold_doc.text)\n",
    "    examples.append(Example(pred_doc, gold_doc))\n",
    "\n",
    "scorer = Scorer()\n",
    "test_scores = scorer.score(examples)\n",
    "\n",
    "print(\"Precision:\", test_scores[\"ents_p\"])\n",
    "print(\"Recall:\", test_scores[\"ents_r\"])\n",
    "print(\"F1:\", test_scores[\"ents_f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e0e5567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-------+\n",
      "| Split | Precision | Recall |   F1  |\n",
      "+-------+-----------+--------+-------+\n",
      "| Train |   0.197   | 0.403  | 0.265 |\n",
      "|  Dev  |   0.168   |  0.31  | 0.218 |\n",
      "|  Test |   0.156   | 0.298  | 0.204 |\n",
      "+-------+-----------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = [\"Split\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "table.add_row([\"Train\",\n",
    "               round(train_scores[\"ents_p\"],3),\n",
    "               round(train_scores[\"ents_r\"],3),\n",
    "               round(train_scores[\"ents_f\"],3)])\n",
    "\n",
    "table.add_row([\"Dev\",\n",
    "               round(dev_scores[\"ents_p\"],3),\n",
    "               round(dev_scores[\"ents_r\"],3),\n",
    "               round(dev_scores[\"ents_f\"],3)])\n",
    "\n",
    "table.add_row([\"Test\",\n",
    "               round(test_scores[\"ents_p\"],3),\n",
    "               round(test_scores[\"ents_r\"],3),\n",
    "               round(test_scores[\"ents_f\"],3)])\n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a358f45",
   "metadata": {},
   "source": [
    "### Training a custom spaCy model\n",
    "\n",
    "The training of a cupstom spaCy NER model can be done both with the command line interface (cli) or in a python script. Using the cli is ususally more optimzed. All the configuration of the training is defined in a coniguration file, which is a good practice for documentation, tracing and reproducibility.\n",
    "\n",
    "The configuration file can be generated on line using the [Quickstart](https://spacy.io/usage/training#quickstart)\n",
    "\n",
    "<img src=\"images/spacy_quickstart.jpg\" width=\"600\" >\n",
    "\n",
    "You can run the training process as a script using the train function (https://spacy.io/usage/training#api-train), specifying the configuration file and the directory in which to save the model as parameters. Once the training is complete, the best and last models are saved in the directory.\n",
    "\n",
    "#### Question\n",
    "> * Generate a training configuration file for a NER in French\n",
    "> * Add the correct path to the training and dev sets generated previously\n",
    "> * train a NER model\n",
    "> * Evaluate the model on the train, dev et test sets. Compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf90601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Saving to output directory: training_output\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     31.93    0.45    1.33    0.27    0.00\n",
      "  0     200         49.08   1895.69   22.43   38.82   15.78    0.22\n",
      "  1     400        321.35   1407.31   31.31   36.27   27.54    0.31\n",
      "  1     600         34.73   1288.48   38.43   46.74   32.62    0.38\n",
      "  2     800         81.49   1377.11   33.94   44.21   27.54    0.34\n",
      "  3    1000       1257.20   1377.33   43.22   52.69   36.63    0.43\n",
      "\u001b[38;5;2m‚úî Saved pipeline to output directory\u001b[0m\n",
      "training_output/model-last\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from spacy.cli.train import train\n",
    "\n",
    "train(\n",
    "    config_path=\"config.cfg\",\n",
    "    output_path=\"training_output\",\n",
    "    use_gpu=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17586b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"training_output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f273520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "def load_spacy_dataset(path, nlp):\n",
    "    doc_bin = DocBin().from_disk(path)\n",
    "    return list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "train_docs = load_spacy_dataset(\"corpus/train.spacy\", nlp)\n",
    "dev_docs   = load_spacy_dataset(\"corpus/dev.spacy\", nlp)\n",
    "test_docs  = load_spacy_dataset(\"corpus/test.spacy\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3982808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:19<00:00, 29.38it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:04<00:00, 31.76it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:03<00:00, 26.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(docs, nlp):\n",
    "    scorer = Scorer()\n",
    "    examples = []\n",
    "\n",
    "    for doc in tqdm(docs):\n",
    "        pred = nlp(doc.text)\n",
    "        examples.append(Example(pred, doc))\n",
    "\n",
    "    scores = scorer.score(examples)\n",
    "    return scores\n",
    "\n",
    "train_scores = evaluate_model(train_docs, nlp)\n",
    "dev_scores   = evaluate_model(dev_docs, nlp)\n",
    "test_scores  = evaluate_model(test_docs, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef9bd0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+-------+\n",
      "| Split | Precision | Recall |   F1  |\n",
      "+-------+-----------+--------+-------+\n",
      "| Train |   0.733   | 0.729  | 0.731 |\n",
      "|  Dev  |   0.525   | 0.366  | 0.431 |\n",
      "|  Test |   0.681   | 0.528  | 0.595 |\n",
      "+-------+-----------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Split\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "table.add_row([\n",
    "    \"Train\",\n",
    "    round(train_scores[\"ents_p\"], 3),\n",
    "    round(train_scores[\"ents_r\"], 3),\n",
    "    round(train_scores[\"ents_f\"], 3)\n",
    "])\n",
    "\n",
    "table.add_row([\n",
    "    \"Dev\",\n",
    "    round(dev_scores[\"ents_p\"], 3),\n",
    "    round(dev_scores[\"ents_r\"], 3),\n",
    "    round(dev_scores[\"ents_f\"], 3)\n",
    "])\n",
    "\n",
    "table.add_row([\n",
    "    \"Test\",\n",
    "    round(test_scores[\"ents_p\"], 3),\n",
    "    round(test_scores[\"ents_r\"], 3),\n",
    "    round(test_scores[\"ents_f\"], 3)\n",
    "])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f34bfb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:01<00:00, 66.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19955654101995565"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare with pretrain for f1\n",
    "nlp_pretrained = spacy.load(\"fr_core_news_lg\")\n",
    "test_scores_pretrained = evaluate_model(test_docs, nlp_pretrained)\n",
    "test_scores_pretrained['ents_f']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34932c83",
   "metadata": {},
   "source": [
    "### Zero-shot NER prediction with GLiNER\n",
    "\n",
    "\n",
    "[GLiNER](https://github.com/fastino-ai/GLiNER2/tree/main)  is a library that provides models for zero-shot named entity recognition. This means that[structured information extraction](https://github.com/fastino-ai/GLiNER2/blob/main/tutorial/3-json_extraction.md)structured information extraction, which means that the extracted information can be organised in a structured JSON format. GLiNER does not provide the location of entities in the text by default, but you can configure the model to output this information (```include_spans=True```). Finally, GLiNER enables entities to be overlapped and nested, which is not supported by the spaCy scorer. The spaCy [filter_spans](https://spacy.io/api/top-level#util.filter_spans) function can be used to remove overlapping entities for evaluation.\n",
    "\n",
    "#### Question\n",
    "> * Define the entities to extract from the text.\n",
    "> * Apply GLiNER on the dev and test sets\n",
    "> * Evaluate the models on the dev and test sets and compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb5fc733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_tags = []\n",
    "for example in ds[\"train\"]:\n",
    "    all_tags.extend(example[\"ner_tags\"])\n",
    "\n",
    "unique_tags = sorted(set(all_tags))\n",
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f09c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type extractor to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß† Model Configuration\n",
      "============================================================\n",
      "Encoder model      : microsoft/deberta-v3-base\n",
      "Counting layer     : count_lstm_v2\n",
      "Token pooling      : first\n",
      "============================================================\n",
      "Precision: 0.288\n",
      "Recall: 0.39\n",
      "F1: 0.331\n"
     ]
    }
   ],
   "source": [
    "from gliner2 import GLiNER2\n",
    "extractor = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\")\n",
    "\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.blank(\"fr\")  # tokenizer only\n",
    "\n",
    "doc_bin = DocBin().from_disk(\"corpus/dev.spacy\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "label_map = {\n",
    "    \"PER\": \"nom de personne dans un document administratif\",\n",
    "    \"ORG\": \"organisation administrative ou institution\",\n",
    "    \"LOC\": \"lieu g√©ographique ou adresse postale\"\n",
    "}\n",
    "\n",
    "gliner_labels = list(label_map.values())\n",
    "reverse_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "examples = []\n",
    "\n",
    "for gold_doc in gold_docs:\n",
    "    text = gold_doc.text\n",
    "    \n",
    "    predictions = extractor.extract_entities(text, gliner_labels, include_spans=True)\n",
    "    pred_doc = nlp.make_doc(text)\n",
    "\n",
    "    spans = []\n",
    "    for gliner_label, entities in predictions[\"entities\"].items():\n",
    "        spacy_label = reverse_map.get(gliner_label)\n",
    "        if not spacy_label:\n",
    "            continue\n",
    "        for ent in entities:\n",
    "            start = ent[\"start\"]\n",
    "            end = ent[\"end\"]\n",
    "\n",
    "            span = pred_doc.char_span(start, end, label=spacy_label)\n",
    "            if span:\n",
    "                spans.append(span)\n",
    "\n",
    "    spans = filter_spans(spans)\n",
    "    pred_doc.ents = spans\n",
    "    examples.append(Example(pred_doc, gold_doc))\n",
    "\n",
    "# Evaluate\n",
    "scorer = Scorer()\n",
    "scores_dev = scorer.score(examples)\n",
    "\n",
    "print(\"Precision:\", round(scores_dev[\"ents_p\"], 3))\n",
    "print(\"Recall:\", round(scores_dev[\"ents_r\"], 3))\n",
    "print(\"F1:\", round(scores_dev[\"ents_f\"], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af100a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:10<00:00,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.321\n",
      "Recall: 0.371\n",
      "F1: 0.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load TEST dataset\n",
    "doc_bin = DocBin().from_disk(\"corpus/test.spacy\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "examples = []\n",
    "\n",
    "for gold_doc in tqdm(gold_docs):\n",
    "    text = gold_doc.text\n",
    "\n",
    "    predictions = extractor.extract_entities(\n",
    "        text,\n",
    "        gliner_labels,\n",
    "        include_spans=True\n",
    "    )\n",
    "\n",
    "    pred_doc = nlp.make_doc(text)\n",
    "\n",
    "    spans = []\n",
    "    for gliner_label, entities in predictions[\"entities\"].items():\n",
    "        spacy_label = reverse_map.get(gliner_label)\n",
    "        if not spacy_label:\n",
    "            continue\n",
    "\n",
    "        for ent in entities:\n",
    "            start = ent[\"start\"]\n",
    "            end = ent[\"end\"]\n",
    "\n",
    "            span = pred_doc.char_span(start, end, label=spacy_label)\n",
    "            if span:\n",
    "                spans.append(span)\n",
    "\n",
    "    spans = filter_spans(spans)\n",
    "    pred_doc.ents = spans\n",
    "\n",
    "    examples.append(Example(pred_doc, gold_doc))\n",
    "\n",
    "# Evaluate TEST\n",
    "scorer = Scorer()\n",
    "scores_test = scorer.score(examples)\n",
    "\n",
    "print(\"Precision:\", round(scores_test[\"ents_p\"], 3))\n",
    "print(\"Recall:\", round(scores_test[\"ents_r\"], 3))\n",
    "print(\"F1:\", round(scores_test[\"ents_f\"], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01aefcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+--------+-------+\n",
      "| Model  | Split | Precision | Recall |   F1  |\n",
      "+--------+-------+-----------+--------+-------+\n",
      "| GLiNER |  Dev  |   0.288   |  0.39  | 0.331 |\n",
      "| GLiNER |  Test |   0.321   | 0.371  | 0.344 |\n",
      "+--------+-------+-----------+--------+-------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Model\", \"Split\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "# GLiNER\n",
    "table.add_row([\"GLiNER\", \"Dev\",\n",
    "               round(scores_dev[\"ents_p\"], 3),\n",
    "               round(scores_dev[\"ents_r\"], 3),\n",
    "               round(scores_dev[\"ents_f\"], 3)])\n",
    "\n",
    "table.add_row([\"GLiNER\", \"Test\",\n",
    "               round(scores_test[\"ents_p\"], 3),\n",
    "               round(scores_test[\"ents_r\"], 3),\n",
    "               round(scores_test[\"ents_f\"], 3)])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d310e6",
   "metadata": {},
   "source": [
    "### Result from the paper\n",
    "\n",
    "**Tabl3 of the paper**\n",
    "\n",
    "<img src=\"images/table3.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c52c34",
   "metadata": {},
   "source": [
    "**Table4 of the paper**\n",
    "\n",
    "<img src=\"images/table4.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de750d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
