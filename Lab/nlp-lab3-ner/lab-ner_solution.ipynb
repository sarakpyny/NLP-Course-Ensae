{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3cdf0d",
   "metadata": {},
   "source": [
    "# Named-entity Recognition (NER) \n",
    "\n",
    "Named entity recognition is a fundamental task in information extraction from textual documents. While named entities originally corresponded to real-world entities with names (named entities), this concept has been extended to any type of information: it is possible to extract chemical molecules, product numbers, amounts, addresses, etc. In this practical assignment, we will use several named entity extraction libraries in French on a small corpus. The objective is not to train the best possible model, but to test the use of each of these libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7cf35",
   "metadata": {},
   "source": [
    "## The AdminSet dataset\n",
    "The AdminSet dataset is a corpus of administrative documents in French produced by automatic character recognition and manually annotated with named entities. This corpus is quite difficult because the document recognition process produces noisy text (errors due to layout, recognition, fonts, etc.).\n",
    "\n",
    "The paper describing the dataset is available [here](https://hal.science/hal-04855066v1/file/AdminSet_et_AdminBERT__version___preprint.pdf).\n",
    "\n",
    "The corpus is available on HuggingFace: [Adminset-NER](https://huggingface.co/datasets/taln-ls2n/Adminset-NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3cdb46b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 729\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 85\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('taln-ls2n/Adminset-NER')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62af8c6",
   "metadata": {},
   "source": [
    "#### Question\n",
    "> * Compute descriptive statistics on the texts  for each split (train, dev)\n",
    "> * Compute descriptive statistics on the entities for each split (train, dev)\n",
    "> * Compare with the statistics reported in the paper (Table 2)\n",
    "> * Display a couple of random texts with their entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8db4b2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fin, Procès-Verbal, Conseil, communautaire, d...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Monsieur, MORLET, excuse, Monsieur, Christoph...</td>\n",
       "      <td>[B-PER, I-PER, O, B-PER, I-PER, I-PER, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Monsieur, MORLET, annonce, le, décès, de, Mon...</td>\n",
       "      <td>[B-PER, I-PER, O, O, O, O, B-PER, I-PER, I-PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Commentaires, ,, débat, Constatant, qu'il, n'...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Page, 4, sur, 15, &lt;, page, &gt;, 4, &lt;, /, page, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [fin, Procès-Verbal, Conseil, communautaire, d...   \n",
       "1  [Monsieur, MORLET, excuse, Monsieur, Christoph...   \n",
       "2  [Monsieur, MORLET, annonce, le, décès, de, Mon...   \n",
       "3  [Commentaires, ,, débat, Constatant, qu'il, n'...   \n",
       "4  [Page, 4, sur, 15, <, page, >, 4, <, /, page, ...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [B-PER, I-PER, O, B-PER, I-PER, I-PER, O, O, O...  \n",
       "2  [B-PER, I-PER, O, O, O, O, B-PER, I-PER, I-PER...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(ds['train'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cfe49bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[et, L’Office, Communautaire, d’Animations, et...</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Signé, le, 22, février, 2024, Reçu, au, Contr...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Reçu, au, Contrôle, de, légalité, le, 12, déc...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Etaient, absents, et, représentés, Mesdames, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-PER, I-PER, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Commune, d'Ollioules, -, Departement, du, Var...</td>\n",
       "      <td>[B-LOC, I-LOC, O, B-LOC, I-LOC, I-LOC, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [et, L’Office, Communautaire, d’Animations, et...   \n",
       "1  [Signé, le, 22, février, 2024, Reçu, au, Contr...   \n",
       "2  [Reçu, au, Contrôle, de, légalité, le, 12, déc...   \n",
       "3  [Etaient, absents, et, représentés, Mesdames, ...   \n",
       "4  [Commune, d'Ollioules, -, Departement, du, Var...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, B-PER, I-PER, O, O, B...  \n",
       "4  [B-LOC, I-LOC, O, B-LOC, I-LOC, I-LOC, O, O, O...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.DataFrame(ds['validation'])\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84f0642f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((729, 2), (85, 2))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b31e1f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fin, Procès-Verbal, Conseil, communautaire, d...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Monsieur, MORLET, excuse, Monsieur, Christoph...</td>\n",
       "      <td>[B-PER, I-PER, O, B-PER, I-PER, I-PER, O, O, O...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Monsieur, MORLET, annonce, le, décès, de, Mon...</td>\n",
       "      <td>[B-PER, I-PER, O, O, O, O, B-PER, I-PER, I-PER...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Commentaires, ,, débat, Constatant, qu'il, n'...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Page, 4, sur, 15, &lt;, page, &gt;, 4, &lt;, /, page, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [fin, Procès-Verbal, Conseil, communautaire, d...   \n",
       "1  [Monsieur, MORLET, excuse, Monsieur, Christoph...   \n",
       "2  [Monsieur, MORLET, annonce, le, décès, de, Mon...   \n",
       "3  [Commentaires, ,, débat, Constatant, qu'il, n'...   \n",
       "4  [Page, 4, sur, 15, <, page, >, 4, <, /, page, ...   \n",
       "\n",
       "                                            ner_tags  n_tokens  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...        63  \n",
       "1  [B-PER, I-PER, O, B-PER, I-PER, I-PER, O, O, O...        24  \n",
       "2  [B-PER, I-PER, O, O, O, O, B-PER, I-PER, I-PER...        31  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER...        18  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...        41  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute statistics on the number of token in train and validation : min, max, mean std, median\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# for train set\n",
    "train_df[\"n_tokens\"] = train_df[\"tokens\"].apply(len)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53546bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN TOKEN STATS\n",
      "count    729.000000\n",
      "mean      63.367627\n",
      "std       52.705843\n",
      "min       15.000000\n",
      "25%       30.000000\n",
      "50%       45.000000\n",
      "75%       75.000000\n",
      "max      379.000000\n",
      "Name: n_tokens, dtype: float64\n",
      "Median: 45.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN TOKEN STATS\")\n",
    "print(train_df[\"n_tokens\"].describe())\n",
    "print(\"Median:\", train_df[\"n_tokens\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ccf51c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION TOKEN STATS\n",
      "count     85.000000\n",
      "mean      79.835294\n",
      "std       68.679006\n",
      "min       19.000000\n",
      "25%       35.000000\n",
      "50%       50.000000\n",
      "75%       86.000000\n",
      "max      352.000000\n",
      "Name: n_tokens, dtype: float64\n",
      "Median: 50.0\n"
     ]
    }
   ],
   "source": [
    "# for validation set\n",
    "val_df[\"n_tokens\"] = val_df[\"tokens\"].apply(len)\n",
    "\n",
    "print(\"\\nVALIDATION TOKEN STATS\")\n",
    "print(val_df[\"n_tokens\"].describe())\n",
    "print(\"Median:\", val_df[\"n_tokens\"].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083e191",
   "metadata": {},
   "source": [
    "**table2 of the paper**\n",
    "\n",
    "<img src=\"images/paper_table2.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5eb2fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN ENTITY STATS (Token-level)\n",
      "Total entity tokens: 4983\n",
      "Number of entity labels: 6\n",
      "ner_tags\n",
      "I-ORG    1476\n",
      "I-PER    1092\n",
      "B-ORG     770\n",
      "B-PER     764\n",
      "B-LOC     454\n",
      "I-LOC     427\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_tags = train_df[\"ner_tags\"].explode()\n",
    "\n",
    "# remove \"O\"\n",
    "train_entities = train_tags[train_tags != \"O\"]\n",
    "\n",
    "print(\"\\nTRAIN ENTITY STATS (Token-level)\")\n",
    "print(\"Total entity tokens:\", len(train_entities))\n",
    "print(\"Number of entity labels:\", train_entities.nunique())\n",
    "print(train_entities.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83f7a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION ENTITY STATS (Token-level)\n",
      "Total entity tokens: 694\n",
      "Number of entity labels: 6\n",
      "ner_tags\n",
      "I-ORG    203\n",
      "I-PER    138\n",
      "B-PER    124\n",
      "B-ORG    123\n",
      "I-LOC     54\n",
      "B-LOC     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "val_tags = val_df[\"ner_tags\"].explode()\n",
    "\n",
    "val_entities = val_tags[val_tags != \"O\"]\n",
    "\n",
    "print(\"\\nVALIDATION ENTITY STATS (Token-level)\")\n",
    "print(\"Total entity tokens:\", len(val_entities))\n",
    "print(\"Number of entity labels:\", val_entities.nunique())\n",
    "print(val_entities.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe10ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN REAL ENTITIES (Span-level)\n",
      "Total entities: 1988\n",
      "Entity types: ner_tags\n",
      "ORG    770\n",
      "PER    764\n",
      "LOC    454\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_b_entities = train_entities[train_entities.str.startswith(\"B-\")]\n",
    "\n",
    "print(\"\\nTRAIN REAL ENTITIES (Span-level)\")\n",
    "print(\"Total entities:\", len(train_b_entities))\n",
    "print(\"Entity types:\", train_b_entities.str[2:].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f4b35b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION REAL ENTITIES (Span-level)\n",
      "Total entities: 299\n",
      "Entity types: ner_tags\n",
      "PER    124\n",
      "ORG    123\n",
      "LOC     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "val_b_entities = val_entities[val_entities.str.startswith(\"B-\")]\n",
    "\n",
    "print(\"\\nVALIDATION REAL ENTITIES (Span-level)\")\n",
    "print(\"Total entities:\", len(val_b_entities))\n",
    "print(\"Entity types:\", val_b_entities.str[2:].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b001706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Text:\n",
      "Fait à SAINT ETIENNE le 08 juin 2023 ( En deux exemplaires ) Pour le S.A.T.P E 21 olas hors les murs » PLE A S OR s v p» Rue Désiré Claude 4000 anti l. 04 7730 07 secretariatoeadultesprismer Pour LE DEPARTEMENT DE LA LOIRE Le Président M. Georges ZIEGLER S.A.T « Hors les murs » Prisme 21 Loire - 40 rue Désiré Claude 42100 SAINT ETIENNE 2023 / 2024 Page 9 / 10 < page > 9 < / page > LISTE ET COORDONNÉES DES PROFESSIONNELS DU SERVICE HABILITÉS À ACCOMPAGNER M. Y.C.\n",
      "\n",
      "Entities:\n",
      "\n",
      "============================================================\n",
      "Text:\n",
      "Ont été équipées les Maisons de la Communauté Côte- Basque-Adour et Sud Pays Basque , la Villa Tarride et le Centre Technique de l’Environnement .\n",
      "\n",
      "Entities:\n",
      "Communauté Côte- Basque-Adour -> LOC\n",
      "Sud Pays Basque -> LOC\n",
      "Villa Tarride -> LOC\n",
      "Centre Technique de l’Environnement -> LOC\n"
     ]
    }
   ],
   "source": [
    "sample = train_df.sample(2)\n",
    "\n",
    "for i, row in sample.iterrows():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Text:\")\n",
    "    print(\" \".join(row[\"tokens\"]))\n",
    "    \n",
    "    print(\"\\nEntities:\")\n",
    "    \n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "    \n",
    "    for token, tag in zip(row[\"tokens\"], row[\"ner_tags\"]):\n",
    "        \n",
    "        if tag.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                print(\" \".join(current_entity), \"->\", current_label)\n",
    "            current_entity = [token]\n",
    "            current_label = tag[2:]\n",
    "        \n",
    "        elif tag.startswith(\"I-\"):\n",
    "            current_entity.append(token)\n",
    "        \n",
    "        else:\n",
    "            if current_entity:\n",
    "                print(\" \".join(current_entity), \"->\", current_label)\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "    \n",
    "    if current_entity:\n",
    "        print(\" \".join(current_entity), \"->\", current_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebbac3",
   "metadata": {},
   "source": [
    "### Creation of the splits\n",
    "\n",
    "The train_test_split() function from huggingface allow to split a dataset randomly in 2 parts : https://huggingface.co/docs/datasets/v4.5.0/process#split\n",
    "\n",
    "The ```spacy_utils.py``` file contains functions to save a dataset in text format (```save_text```, usefull for inspection), BIO format (```save_bio```) and spacy format (```save_docbin```).\n",
    "\n",
    "#### Questions\n",
    ">* Using the split function, create a train/dev/test split corresponding to the proportions reported in the paper\n",
    ">* Save the sets in a corpus directory, in text, bio and docbin formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07bc8ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814, 2)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy_utils import save_bio, save_text, save_docbin\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "full_ds = concatenate_datasets([ds[\"train\"], ds[\"validation\"]])\n",
    "full_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47c37959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((583, 2), (146, 2), (85, 2))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = full_ds.select(range(0, 583))\n",
    "dev_ds   = full_ds.select(range(583, 583 + 146))\n",
    "test_ds  = full_ds.select(range(583 + 146, 814))\n",
    "\n",
    "train_ds.shape, dev_ds.shape, test_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f099f6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 583\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e92230d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 146\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2891f2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 85\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7585e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving text to corpus/train.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583/583 [00:00<00:00, 9283.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.txt\n",
      "Saving text to corpus/dev.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 12646.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.txt\n",
      "Saving text to corpus/test.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 9055.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.txt\n",
      "Saving BIO text to corpus/train.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583/583 [00:00<00:00, 16618.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.bio\n",
      "Saving BIO text to corpus/dev.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 18605.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.bio\n",
      "Saving BIO text to corpus/test.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 11457.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.bio\n",
      "Creating corpus/train.spacy with 583 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583/583 [00:00<00:00, 5377.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.spacy\n",
      "Creating corpus/dev.spacy with 146 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 4530.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.spacy\n",
      "Creating corpus/test.spacy with 85 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 3595.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy_utils import save_bio, save_text, save_docbin\n",
    "\n",
    "# save the datasets in different formats\n",
    "save_text(train_ds, \"corpus/train.txt\")\n",
    "save_text(dev_ds, \"corpus/dev.txt\")\n",
    "save_text(test_ds, \"corpus/test.txt\")\n",
    "\n",
    "save_bio(train_ds, \"corpus/train.bio\")\n",
    "save_bio(dev_ds, \"corpus/dev.bio\")\n",
    "save_bio(test_ds, \"corpus/test.bio\")\n",
    "\n",
    "save_docbin(train_ds, \"corpus/train.spacy\")\n",
    "save_docbin(dev_ds, \"corpus/dev.spacy\")\n",
    "save_docbin(test_ds, \"corpus/test.spacy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b05a4b",
   "metadata": {},
   "source": [
    "### Testing spaCy pre-trained NER models\n",
    "\n",
    "spaCy comes with a several pretrained models for many languages. For French, 4 models are provided : https://spacy.io/models/fr\n",
    "\n",
    "To apply a pretrained model to dataset, use : \n",
    "- ```nlp = spacy.load(MODEL_NAME)``` to load the model. You need to download it first with \"spacy download MODEL_NAME\"\n",
    "- ```DocBin().from_disk()``` to load a dataset in spaCy format from the disk\n",
    "- ```doc_bin.get_docs(nlp.vocab)``` to convert the dataset from binary to text format\n",
    "- ```nlp(doc.text)```to apply the NER model to a text\n",
    "\n",
    "To evaluate the prediction, you can use the spaCy [Scorer](https://spacy.io/api/scorer)\n",
    "- ```scorer.score(examples)``` where examples is a list of spaCy ```Example(prediction, reference)````\n",
    "\n",
    "#### Question\n",
    "\n",
    ">* Using a spaCy pretrained model for French, evaluate its performace for NER prediction on the train, dev and test sets\n",
    ">* Compare this model to results reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c54891c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nysarakpy/github/NLP-Course-Ensae/.venv/bin/python\n",
      "3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7d9ef205",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'fr_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfr_core_news_md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/NLP-Course-Ensae/.venv/lib/python3.12/site-packages/spacy/__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/NLP-Course-Ensae/.venv/lib/python3.12/site-packages/spacy/util.py:531\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'fr_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b8524",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'fr_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprettytable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrettyTable\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfr_core_news_md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/NLP-Course-Ensae/.venv/lib/python3.12/site-packages/spacy/__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/NLP-Course-Ensae/.venv/lib/python3.12/site-packages/spacy/util.py:531\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'fr_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a358f45",
   "metadata": {},
   "source": [
    "### Training a custom spaCy model\n",
    "\n",
    "The training of a cupstom spaCy NER model can be done both with the command line interface (cli) or in a python script. Using the cli is ususally more optimzed. All the configuration of the training is defined in a coniguration file, which is a good practice for documentation, tracing and reproducibility.\n",
    "\n",
    "The configuration file can be generated on line using the [Quickstart](https://spacy.io/usage/training#quickstart)\n",
    "\n",
    "<img src=\"images/spacy_quickstart.jpg\" width=\"600\" >\n",
    "\n",
    "You can run the training process as a script using the train function (https://spacy.io/usage/training#api-train), specifying the configuration file and the directory in which to save the model as parameters. Once the training is complete, the best and last models are saved in the directory.\n",
    "\n",
    "#### Question\n",
    "> * Generate a training configuration file for a NER in French\n",
    "> * Add the correct path to the training and dev sets generated previously\n",
    "> * train a NER model\n",
    "> * Evaluate the model on the train, dev et test sets. Compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from spacy.cli.train import train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34932c83",
   "metadata": {},
   "source": [
    "### Zero-shot NER prediction with GLiNER\n",
    "\n",
    "\n",
    "[GLiNER](https://github.com/fastino-ai/GLiNER2/tree/main)  is a library that provides models for zero-shot named entity recognition. This means that[structured information extraction](https://github.com/fastino-ai/GLiNER2/blob/main/tutorial/3-json_extraction.md)structured information extraction, which means that the extracted information can be organised in a structured JSON format. GLiNER does not provide the location of entities in the text by default, but you can configure the model to output this information (```include_spans=True```). Finally, GLiNER enables entities to be overlapped and nested, which is not supported by the spaCy scorer. The spaCy [filter_spans](https://spacy.io/api/top-level#util.filter_spans) function can be used to remove overlapping entities for evaluation.\n",
    "\n",
    "#### Question\n",
    "> * Define the entities to extract from the text.\n",
    "> * Apply GLiNER on the dev and test sets\n",
    "> * Evaluate the models on the dev and test sets and compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner2 import GLiNER2\n",
    "extractor = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\")\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.blank(\"fr\")  # tokenizer only\n",
    "\n",
    "doc_bin = DocBin().from_disk(\"REPLACE\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "label_map = {\n",
    "    # Define the entities here\n",
    "}\n",
    "\n",
    "gliner_labels = list(label_map.values())\n",
    "reverse_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "examples = []\n",
    "\n",
    "for gold_doc in gold_docs:\n",
    "    text = gold_doc.text\n",
    "    \n",
    "    predictions = extractor.extract_entities(text, gliner_labels, include_spans=True)\n",
    "    pred_doc = nlp.make_doc(text)\n",
    "\n",
    "    spans = []\n",
    "    for gliner_label, entities in predictions[\"entities\"].items():\n",
    "        spacy_label = reverse_map.get(gliner_label)\n",
    "        if not spacy_label:\n",
    "            continue\n",
    "        for ent in entities:\n",
    "            start = ent[\"start\"]\n",
    "            end = ent[\"end\"]\n",
    "\n",
    "            span = pred_doc.char_span(start, end, label=spacy_label)\n",
    "            if span:\n",
    "                spans.append(span)\n",
    "\n",
    "    spans = filter_spans(spans)\n",
    "    pred_doc.ents = spans\n",
    "    examples.append(Example(pred_doc, gold_doc))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
